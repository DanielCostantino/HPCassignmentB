plot(t)
text(t)
nrow(tree(Species~., d)$frame)
computeComplexityOfPerfectTree = function (y.name, data) {
t = tree(formula(paste0(y.name,"~.")), data, mindev=0, minsize=2)
nrow(t$frame)
}
computeComplexityOfPerfectTree("Species", d)
for (variable in names(d)) {
complexity = computeComplexityOfPerfectTree(variable, d)
cat(paste(variable, complexity, "\n"))
}
require(tidyverse)
d = iris
summary(d)
summary(d[d$Species!="setosa",])
summary(subset(d, d$Species!="setosa"))
d = d %>% filter(Species!="setosa")
summary(d)
require(randomForest)
m = randomForest(Species~., d)
summary(d$Species)
print(levels(d$Species))
print(unique(d$Species))
print(levels(d$Species))
print(levels(factor(d$Species)))
print(levels(droplevels(d$Species)))
d$Species = factor(d$Species)
summary(d)
d$Species = factor(d$Species)
summary(d)
m = randomForest(Species~., d)
m
m$confusion
head(m$err.rate)
data<-read.csv("https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni.csv")
data<-data[data$denominazione_regione=="Toscana",]
View(data)
data<-data[,colSums(is.na(data))==0] #metto a 0 i valori NULL
data<-read.csv("https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni.csv")
data<-data[data$denominazione_regione=="Toscana",]
help(is.na)
data<-data[,colSums(is.na(data))==0] # salvo solo le colonne che non hanno valori nulli
data<-data[,-c(2,3,4,5,6)]
data<-data[,-c(13:17)]
data<-data[,-1] #elimino colonna data
data<-data[,-c(1,4,5,9:11)] #dataframe dopo ANALISI DATI
data<-data[221:358,] #01/10/20 -> 01/02/21
data<-read.csv("https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni.csv")
data<-data[data$denominazione_regione=="Toscana",]
data<-data[,colSums(is.na(data))==0] # salvo solo le colonne che non hanno valori nulli
data<-data[,-c(2,3,4,5,6)]
data<-data[,-c(13:17)]
data<-data[,-1] #elimino colonna data
data<-data[,-c(1,4,5,9:11)] #dataframe dopo ANALISI DATI
data<-data[221:358,] #01/10/20 -> 01/02/21
#aggiungo dati(colonna) COLORI partial lockdown
color='miao'
color[1:32]='yellow'
color[33:53]='red'
color[54:81]='orange'
color[82:95]='yellow'
color[96:102]='orange'
color[103:109]='yellow'
color[110:138]='orange'
color=as.factor(color)
data$colori=color
datatrain=data[1:124,]
datatest=data[125:138,]
View(data)
##<!-----------GLM1: SINGLE COVARINT----------------->
####################
##### COMMENTI #####
####################
# Conosciamo già che per "definizione" abbiamo una relazione di linearità tra
# terapia_intensiva e TOTALE_OSPEDALIZZATI
glm1 <- glm(terapia_intensiva ~ totale_ospedalizzati, data = datatrain, family = poisson)
summary(glm1)
# Call:
#   glm(formula = terapia_intensiva ~ totale_ospedalizzati, family = poisson,
#       data = datatrain)
#
# Deviance Residuals:
#   Min       1Q   Median       3Q      Max
# -5.5467  -1.8942   0.0882   1.8422   3.2845
#
# Coefficients:
#   Estimate Std. Error z value Pr(>|z|)
# (Intercept)          3.966e+00  1.940e-02  204.50   <2e-16 ***
#   totale_ospedalizzati 8.875e-04  1.285e-05   69.04   <2e-16 ***
#   ---
#   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#
# (Dispersion parameter for poisson family taken to be 1)
#
# NULL DEVIANCE:
#(=distanza/variabilità dal "null model"): 5673.71  on 123  degrees of freedom
# RESIDUAL DEVIANCE:
#(=distanza/variabilità dal "saturated model"):  662.87  on 122  degrees of freedom
# AIC: 1503.2
#
# Number of Fisher Scoring iterations: 4
####ANALISI RESIDUI GLM1:
plot(glm1)  #plot.lm -> Plot Diagnostics for an lm Object
#### PLOTTING ACTUAL vs FITTED VALUES:
plot(datatrain[,"terapia_intensiva"] ~ predict.glm(glm1, newdata = datatrain, type="response"), ylab="Actual values", xlab="Fitted values")
abline(0,1, col="red", lty="solid")
##############################
###### PREDICTION ERROR ######
##############################
prediction <- predict.glm(glm1, newdata = datatest, type="response")
# Mean Absolute Error(MAE):
# it is the average absolute difference between observed and predicted outcomes,
# MAE = mean(abs(observeds - predicteds)).
MAE = mean(abs(prediction-datatest[,"terapia_intensiva"]))
MAE
# [1] 9.664959
# Mean Squared Error(MSE):
MSE = mean((prediction - datatest[,"terapia_intensiva"])^2)
MSE
#[1] 33.33847 <-- the best togheter with model gml8
# MAPE = (1/n) * sum(|Original-Predicted|/|Original|) * 100
# Why MAPE?
#  MAPE is one of the easiest methods and easy to infer and explain.
#  Suppose MAPE value of a particular model is 5% indicate that
#  the average difference between the predicted value and the original value is 5%.
MAPE = (1/nrow(datatest))*sum(abs(datatest[,"terapia_intensiva"]-prediction) / abs(datatest[,"terapia_intensiva"])) *100
MAPE
data<-read.csv("https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni.csv")
data<-data[data$denominazione_regione=="Toscana",]
data<-data[,colSums(is.na(data))==0] # salvo solo le colonne che non hanno valori nulli
data<-data[,-c(2,3,4,5,6)]
data<-data[,-c(13:17)]
data<-data[,-1] #elimino colonna data
data<-data[,-c(1,4,5,9:11)] #dataframe dopo ANALISI DATI
data<-data[221:358,] #01/10/20 -> 01/02/21
#aggiungo dati(colonna) COLORI partial lockdown
color='miao'
color[1:32]='yellow'
color[33:53]='red'
color[54:81]='orange'
color[82:95]='yellow'
color[96:102]='orange'
color[103:109]='yellow'
color[110:138]='orange'
color=as.factor(color)
data$colori=color
datatrain=data[1:124,]
datatest=data[125:138,]
View(data)
View(datatest)
glm1 <- glm(terapia_intensiva ~ totale_ospedalizzati, data = datatrain, family = poisson)
summary(glm1)
##############################
###### PREDICTION ERROR ######
##############################
prediction <- predict.glm(glm1, newdata = datatest, type="response")
# Mean Absolute Error(MAE):
# it is the average absolute difference between observed and predicted outcomes,
# MAE = mean(abs(observeds - predicteds)).
MAE = mean(abs(prediction-datatest[,"terapia_intensiva"]))
MAE
# Mean Squared Error(MSE):
MSE = mean((prediction - datatest[,"terapia_intensiva"])^2)
MSE
MAPE = (1/nrow(datatest))*sum(abs(datatest[,"terapia_intensiva"]-prediction) / abs(datatest[,"terapia_intensiva"])) *100
MAPE
###########################################
############## QUASI gml1 #################
###########################################
glm1quasi <- glm(terapia_intensiva ~ totale_ospedalizzati, data = datatrain, family = poisson)
summary(glm1quasi)
###########################################
############## QUASI gml1 #################
###########################################
glm1quasi <- glm(terapia_intensiva ~ totale_ospedalizzati, data = datatrain, family = quasipoisson)
summary(glm1quasi)
#############################################################################################################################
#<!-----------GLM2: SINGLE COVARIANT con LOG TRASFORMATION----------------->
#############################################################################################################################
####################
##### COMMENTI #####
####################
# annulliamo "l'effetto" del logaritmo applicato alla MEDIA della response (cioè dell'esponenziale applicato al "linear predictor" B_0 + B_1*):
# in breve annulliamo la "link function"
glm2 <- glm(terapia_intensiva ~ log(totale_ospedalizzati), data = datatrain, family = poisson)
summary(glm2)
#
# Call:
#   glm(formula = terapia_intensiva ~ log(totale_ospedalizzati),
#       family = poisson, data = datatrain)
#
# Deviance Residuals (Rd_i):
#   Min        1Q    Median        3Q       Max
# -2.04654  -0.91754   0.09492   0.80944   1.85540
#
# Coefficients:
#   Estimate Std. Error z value Pr(>|z|)
# (Intercept)               -1.66587    0.10793  -15.43   <2e-16 ***
#   log(totale_ospedalizzati)  0.96274    0.01504   64.00   <2e-16 ***
#   ---
#   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#
# (Dispersion parameter for poisson family taken to be 1)
#
# NULL DEVIANCE: 5673.71  on 123  degrees of freedom
# RESIDUAL DEVIANCE:  126.27  on 122  degrees of freedom
# AIC: 966.6
#
# Number of Fisher Scoring iterations: 4
####################
##### COMMENTI #####
####################
#notiamo che
#  la NULL DEVIANCE è rimasta costante;
#  mentre la RESIDUAL DEVIANCE è diminuita di molto, dimostrando un miglior "fitting" dei dati;
#  i DEVIANCE RESIDUALS (Rd_i) sono più vicini a zero(meno varianza);
####ANALISI RESIDUI GLM2:
plot(glm2) # residuals (deviance and person) vs log(u_i) = eta != u_i
####################
##### COMMENTI #####
####################
# RESIDUALS vs ETA
#   ?????? -> EGIDI
#
# SCALE-LOCATION
#  si rovina nella parte finale
#
# NORMALITY OF PEARSON RESIDUALS (grafico NORMALqq):
#  è buona, sopratutto al centro con leggere deviazioni agli estremi;
#
# OUTLIERS (residuals vs leverage):
#   dal grafico RESIDUALS vs LEVERAGE che sia la leverage che la cook's distance sono basse per tutti i valori;
#### PLOTTING THE FITTED MODEL:
# plot(1:124, datatrain[,"terapia_intensiva"], pch = 16, xlab = "Days", ylab = "Terapia intensive")
# points(1:124, predict(glm2, type="response"), col = 2, pch = 16)
# curve(predict(glm2, data.frame(dist=x)), col="red", lty="solid", lwd=2, add=TRUE)
plot(1:124, datatrain[,"terapia_intensiva"], pch = 16, xlab = "Days", ylab = "Terapia intensive")
lines(fitted(glm2, type="response"), col = 2)
#### PLOTTING ACTUAL vs FITTED VALUES:
plot(datatrain[,"terapia_intensiva"] ~ predict.glm(glm2, newdata = datatrain, type="response"), ylab="Actual values", xlab="Fitted values")
abline(0,1, col="red", lty="solid")
##############################
###### PREDICTION ERROR ######
##############################
prediction <- predict.glm(glm2, newdata = datatest, type="response")
# Mean Absolute Error(MAE):
# it is the average absolute difference between observed and predicted outcomes,
# MAE = mean(abs(observeds - predicteds)).
MAE = mean(abs(prediction-datatest[,"terapia_intensiva"]))
MAE
# [1] 5.303958
# Mean Squared Error(MSE):
MSE = mean((prediction - datatest[,"terapia_intensiva"])^2)
MSE
# [1] 34.66384
prediction <- predict.glm(glm1quasi, newdata = datatest, type="response")
# Mean Absolute Error(MAE):
# it is the average absolute difference between observed and predicted outcomes,
# MAE = mean(abs(observeds - predicteds)).
MAE = mean(abs(prediction-datatest[,"terapia_intensiva"]))
MAE
# [1] 9.664959
# Mean Squared Error(MSE):
MSE = mean((prediction - datatest[,"terapia_intensiva"])^2)
MSE
#[1] 138.6252
# MAPE = (1/n) * sum(|Original-Predicted|/|Original|) * 100
# Why MAPE?
#  MAPE is one of the easiest methods and easy to infer and explain.
#  Suppose MAPE value of a particular model is 5% indicate that
#  the average difference between the predicted value and the original value is 5%.
MAPE = (1/nrow(datatest))*sum(abs(datatest[,"terapia_intensiva"]-prediction) / abs(datatest[,"terapia_intensiva"])) *100
MAPE
prediction <- predict.glm(glm2, newdata = datatest, type="response")
prediction
prediction-datatest[,"terapia_intensiva"]
mean(prediction-datatest[,"terapia_intensiva"])
##############################
###### PREDICTION ERROR ######
##############################
prediction <- predict.glm(glm1, newdata = datatest, type="response")
MAPE = (1/nrow(datatrain))*sum(abs(datatrain[,"terapia_intensiva"]-prediction) / abs(datatrain[,"terapia_intensiva"])) *100
MAPE
##############################
###### PREDICTION ERROR ######
##############################
prediction <- predict.glm(glm1, newdata = datatrain, type="response")
MAPE = (1/nrow(datatrain))*sum(abs(datatrain[,"terapia_intensiva"]-prediction) / abs(datatrain[,"terapia_intensiva"])) *100
MAPE
####ANALISI RESIDUI GLM1:
plot(glm1)  #plot.lm -> Plot Diagnostics for an lm Object
plot(1:124, datatrain[,"terapia_intensiva"], pch = 16, xlab = "Days", ylab = "Terapia intensive")
lines(fitted(glm1, type="response"), col = 2)
#### PLOTTING ACTUAL vs FITTED VALUES:
plot(datatrain[,"terapia_intensiva"] ~ predict.glm(glm1, newdata = datatrain, type="response"), ylab="Actual values", xlab="Fitted values")
abline(0,1, col="red", lty="solid")
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#
# (Dispersion parameter for quasipoisson family taken to be 5.028985) !!!!!!! the DISPERSION PARAMETER can be obtain as the RESIDUAL DEVIANCE/DEGREES OF FREEDOM = 5.43
#
# Null deviance: 5673.71  on 123  degrees of freedom
# Residual deviance:  662.87  on 122  degrees of freedom
# AIC: NA
#
# Number of Fisher Scoring iterations: 4
plot(glm1quasi)
plot(residuals(glm1,type = "response"))
plot(residuals(glm1,type = "pearson")) # response residuals scaled respect the sd [vs log(u_i)]
plot(residuals(glm1,type = "deviance")) # similar to pearson res. for n large [vs log(u_i)]
help(residuals)
help(residuals.glm)
####ANALISI RESIDUI GLM1:
plot(glm1)  #plot.lm -> Plot Diagnostics for an lm Object
glm7 <- glm(terapia_intensiva ~ log(totale_ospedalizzati) + colori + log(totale_ospedalizzati):colori + log(nuovi_positivi) + log(dimessi_guariti) + log(variazione_totale_positivi), data = datatrain, family = quasipoisson)
glm7 <- glm(terapia_intensiva ~ log(totale_ospedalizzati) + colori + log(totale_ospedalizzati):colori + log(nuovi_positivi) + log(dimessi_guariti) + variazione_totale_positivi, data = datatrain, family = quasipoisson)
glm2 <- glm(terapia_intensiva ~ log(totale_ospedalizzati), data = datatrain, family = quasipoisson)
summary(glm2)
#
# Call:
#   glm(formula = terapia_intensiva ~ log(totale_ospedalizzati),
#       family = poisson, data = datatrain)
#
# Deviance Residuals (Rd_i):
#   Min        1Q    Median        3Q       Max
# -2.04654  -0.91754   0.09492   0.80944   1.85540
#
# Coefficients:
#   Estimate Std. Error z value Pr(>|z|)
# (Intercept)               -1.66587    0.10793  -15.43   <2e-16 ***
#   log(totale_ospedalizzati)  0.96274    0.01504   64.00   <2e-16 ***
#   ---
#   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#
# (Dispersion parameter for poisson family taken to be 1)
#
# NULL DEVIANCE: 5673.71  on 123  degrees of freedom
# RESIDUAL DEVIANCE:  126.27  on 122  degrees of freedom
# AIC: 966.6
#
# Number of Fisher Scoring iterations: 4
####################
##### COMMENTI #####
####################
#notiamo che
#  la NULL DEVIANCE è rimasta costante;
#  mentre la RESIDUAL DEVIANCE è diminuita di molto, dimostrando un miglior "fitting" dei dati;
#  i DEVIANCE RESIDUALS (Rd_i) sono più vicini a zero(meno varianza);
####ANALISI RESIDUI GLM2:
plot(glm2) # residuals (deviance and person) vs log(u_i) = eta != u_i
prediction <- predict.glm(glm2, newdata = datatest, type="response")
# Mean Absolute Error(MAE):
# it is the average absolute difference between observed and predicted outcomes,
# MAE = mean(abs(observeds - predicteds)).
MAE = mean(abs(prediction-datatest[,"terapia_intensiva"]))
MAE
# [1] 5.303958
# Mean Squared Error(MSE):
MSE = mean((prediction - datatest[,"terapia_intensiva"])^2)
MSE
# [1] 34.66384
glm4 <- glm(terapia_intensiva ~ log(totale_ospedalizzati) + colori, data = datatrain, family = quasipoisson)
summary(glm4)
# Call:
#   glm(formula = terapia_intensiva ~ log(totale_ospedalizzati) +
#         colori, family = poisson, data = datatrain)
#
# Deviance Residuals:
#   Min        1Q    Median        3Q       Max
# -1.99710  -0.60502   0.06343   0.74811   2.16226
#
# Coefficients:
#                             Estimate  Std. Error    z  value Pr(>|z|)
#   (Intercept)               -1.88548    0.14535 -12.972  < 2e-16 ***
#   log(totale_ospedalizzati)  0.99754    0.02009  49.648  < 2e-16 ***
#   colorired                 -0.09606    0.01830  -5.250 1.52e-07 ***
#   coloriyellow              -0.01458    0.01993  -0.731    0.464
# ---
#   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#
# (Dispersion parameter for poisson family taken to be 1)
#
# Null deviance: 5673.708  on 123  degrees of freedom
# Residual deviance:   98.514  on 120  degrees of freedom
# AIC: 942.84
#
# Number of Fisher Scoring iterations: 4
plot(glm4)
glm4 <- glm(terapia_intensiva ~ log(totale_ospedalizzati) + colori, data = datatrain, family = quasipoisson)
summary(glm4)
glm2 <- glm(terapia_intensiva ~ log(totale_ospedalizzati), data = datatrain, family = quasipoisson)
summary(glm2)
# curve(predict(glm2, data.frame(dist=x)), col="red", lty="solid", lwd=2, add=TRUE)
plot(1:124, datatrain[,"terapia_intensiva"], pch = 16, xlab = "Days", ylab = "Terapia intensive")
lines(fitted(glm2, type="response"), col = 2)
#### PLOTTING ACTUAL vs FITTED VALUES:
plot(datatrain[,"terapia_intensiva"] ~ predict.glm(glm2, newdata = datatrain, type="response"), ylab="Actual values", xlab="Fitted values")
abline(0,1, col="red", lty="solid")
data<-read.csv("https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni.csv")
data<-data[data$denominazione_regione=="Toscana",]
data<-data[,colSums(is.na(data))==0] # salvo solo le colonne che non hanno valori nulli
data<-data[,-c(2,3,4,5,6)]
data<-data[,-c(13:17)]
data<-data[,-1] #elimino colonna data
data<-data[,-c(1,4,5,9:11)] #dataframe dopo ANALISI DATI
data<-data[221:358,] #01/10/20 -> 01/02/21
#aggiungo dati(colonna) COLORI partial lockdown
color='miao'
color[1:32]='yellow'
color[33:53]='red'
color[54:81]='orange'
color[82:95]='yellow'
color[96:102]='orange'
color[103:109]='yellow'
color[110:138]='orange'
color=as.factor(color)
data$colori=color
datatrain=data[1:124,]
datatest=data[125:138,]
glm1 <- glm(terapia_intensiva ~ totale_ospedalizzati, data = datatrain, family = poisson)
summary(glm1)
plot(residuals(glm1,type = "pearson")) # response residuals scaled respect the sd [vs log(u_i) = n_i]
plot(residuals(glm1,type = "deviance"))
plot(residuals(glm1) ~ predict(glm1,type="response"),xlab=expression(hat(mu)),ylab="Deviance residuals",pch=20,col="red")
plot(residuals(glm1) ~ predict(glm1,type="link"),xlab=expression(hat(mu)),ylab="Deviance residuals",pch=20,col="red")
###########################################
############## QUASI gml1 #################
###########################################
glm1quasi <- glm(terapia_intensiva ~ totale_ospedalizzati, data = datatrain, family = quasipoisson)
plot(residuals(glm1quasi) ~ predict(glm1quasi,type="link"),xlab=expression(hat(mu)),ylab="Deviance residuals",pch=20,col="red")
plot(residuals(glm1,type = "pearson") ~ predict(glm1,type="link"),xlab=expression(hat(mu)),ylab="Deviance residuals",pch=20,col="red")
plot(residuals(glm1quasi,type = "pearson")) ~ predict(glm1quasi,type="link"),xlab=expression(hat(mu)),ylab="Deviance residuals",pch=20,col="red")
plot(residuals(glm1quasi,type = "pearson") ~ predict(glm1quasi,type="link"),xlab=expression(hat(mu)),ylab="Deviance residuals",pch=20,col="red")
setwd("C:/Users/black/Documents/GitHub/HPCassignmentB")
dataN <- read.delim("tempiN.txt", header = FALSE, sep = " ", dec = ".")
#View(dataN)
dataN <- dataN[,-ncol(dataN)]
View(dataN)
#data <- read.delim("tempi.txt", header = FALSE, sep = ",", dec = ".")
#View(data)
#Add header to dataframe
colnames(dataN) <- c("2","4","6","8","10","12","14","16","18","20")
# Mean
mean_dataN <- colMeans(dataN[sapply(dataN, is.numeric)])
mean_dataN
plot(colnames(dataN), mean_dataN,  xlab="N", ylab="Wall-Clock Time [s]" , type="b", pch=22, col="green")
#Np
dataNp <- read.delim("tempiNp.txt", header = FALSE, sep = " ", dec = ".")
#Np
dataNp <- read.delim("tempiNp.txt", header = FALSE, sep = " ", dec = ".")
dataNp <- dataNp[,-ncol(dataNp)]
View(dataNp)
#Add header to dataframe
#colnames(dataNp) <- c("2","4","6","8","10","12","14","16","18","20")
# Mean
mean_dataNp <- colMeans(dataNp[sapply(dataNp, is.numeric)])
mean_dataNp
plot(i, mean_dataNp,  xlab="N", ylab="Wall-Clock Time [s]" , type="b", pch=22, col="yellow")
dataNp <- read.delim("tempiNp.txt", header = FALSE, sep = " ", dec = ".")
#View(dataN)
dataNp <- dataNp[,-ncol(dataNp)]
View(dataNp)
#Add header to dataframe
#colnames(dataNp) <- c("2","4","6","8","10","12","14","16","18","20")
# Mean
mean_dataNp <- colMeans(dataNp[sapply(dataNp, is.numeric)])
mean_dataNp
#Np
dataNp <- read.delim("tempiNp.txt", header = FALSE, sep = " ", dec = ".")
#View(dataN)
dataNp <- dataNp[,-ncol(dataNp)]
View(dataNp)
for (i in 1:50) {
v[i] = i*10
}
v <- c("")
for (i in 1:50) {
v[i] = i*10
}
v
v <- c()
for (i in 1:50) {
v[i] = i*10
}
v
colnames(dataNp) <- v
#Add header to dataframe
#colnames(dataNp) <- c("2","4","6","8","10","12","14","16","18","20")
# Mean
mean_dataNp <- colMeans(dataNp[sapply(dataNp, is.numeric)])
mean_dataNp
plot(colnames(dataNp), mean_dataNp,  xlab="N", ylab="Wall-Clock Time [s]" , type="b", pch=22, col="yellow")
plot(colnames(dataNp), mean_dataNp,  xlab="N", ylab="Wall-Clock Time [s]" , type="b", pch=22, col="red")
dataNp <- read.delim("tempiNp.txt", header = FALSE, sep = " ", dec = ".")
#View(dataN)
dataNp <- dataNp[,-ncol(dataNp)]
View(dataNp)
v <- c()
for (i in 1:50) {
v[i] = i*10
}
v
colnames(dataNp) <- v
#Add header to dataframe
#colnames(dataNp) <- c("2","4","6","8","10","12","14","16","18","20")
# Mean
mean_dataNp <- colMeans(dataNp[sapply(dataNp, is.numeric)])
mean_dataNp
plot(colnames(dataNp), mean_dataNp,  xlab="N", ylab="Wall-Clock Time [s]" , type="b", pch=22, col="red")
plot(colnames(dataNp), mean_dataNp,  xlab="Np", ylab="Wall-Clock Time [s]" , type="b", pch=22, col="red")
plot(colnames(dataN), mean_dataN,  xlab="N", ylab="Wall-Clock Time [s]" , type="b", pch=22, col="blue")
